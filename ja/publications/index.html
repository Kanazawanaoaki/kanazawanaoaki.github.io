<!doctype html><html lang=ja><head><title>Publications · 金沢 直晃</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=color-scheme content="light dark"><meta name=author content="金沢 直晃"><meta name=description content="International Journal Papers 見出しへのリンク Y. Obinata, K. Kawaharazuka, N. Kanazawa, N. Yamaguchi, N. Tsukamoto, I. Yanokura, S. Kitagawa, K. Okada, M. Inaba. &ldquo;Situation classification of living environment by daily life support robot using pre-trained large-scale vision-language model&rdquo;, Advanced Robotics (AR), vol.39, no.7, pp.323-337, 2025. Paper Link
N. Kanazawa, K. Kawaharazuka, Y. Obinata, K. Okada, M. Inaba. &ldquo;Real-world cooking robot system from recipes based on food state recognition using foundation models and PDDL&rdquo;, Advanced Robotics (AR), vol."><meta name=keywords content="blog,desenvolvedor,pessoal"><meta name=twitter:card content="summary"><meta name=twitter:title content="Publications"><meta name=twitter:description content="International Journal Papers 見出しへのリンク Y. Obinata, K. Kawaharazuka, N. Kanazawa, N. Yamaguchi, N. Tsukamoto, I. Yanokura, S. Kitagawa, K. Okada, M. Inaba. &ldquo;Situation classification of living environment by daily life support robot using pre-trained large-scale vision-language model&rdquo;, Advanced Robotics (AR), vol.39, no.7, pp.323-337, 2025. Paper Link
N. Kanazawa, K. Kawaharazuka, Y. Obinata, K. Okada, M. Inaba. &ldquo;Real-world cooking robot system from recipes based on food state recognition using foundation models and PDDL&rdquo;, Advanced Robotics (AR), vol."><meta property="og:title" content="Publications"><meta property="og:description" content="International Journal Papers 見出しへのリンク Y. Obinata, K. Kawaharazuka, N. Kanazawa, N. Yamaguchi, N. Tsukamoto, I. Yanokura, S. Kitagawa, K. Okada, M. Inaba. &ldquo;Situation classification of living environment by daily life support robot using pre-trained large-scale vision-language model&rdquo;, Advanced Robotics (AR), vol.39, no.7, pp.323-337, 2025. Paper Link
N. Kanazawa, K. Kawaharazuka, Y. Obinata, K. Okada, M. Inaba. &ldquo;Real-world cooking robot system from recipes based on food state recognition using foundation models and PDDL&rdquo;, Advanced Robotics (AR), vol."><meta property="og:type" content="article"><meta property="og:url" content="https://kanazawanaoaki.github.io/ja/publications/"><meta property="article:section" content><meta property="og:site_name" content="金沢 直晃"><link rel=canonical href=https://kanazawanaoaki.github.io/ja/publications/><link rel=preload href="/fonts/forkawesome-webfont.woff2?v=1.2.0" as=font type=font/woff2 crossorigin><link rel=stylesheet href=/css/coder.min.ea4c355c5f9913809f506132a80bf3fab84f2679dee370f334f7385a36d24c38.css integrity="sha256-6kw1XF+ZE4CfUGEyqAvz+rhPJnne43DzNPc4WjbSTDg=" crossorigin=anonymous media=screen><link rel=stylesheet href=/css/coder-dark.min.a00e6364bacbc8266ad1cc81230774a1397198f8cfb7bcba29b7d6fcb54ce57f.css integrity="sha256-oA5jZLrLyCZq0cyBIwd0oTlxmPjPt7y6KbfW/LVM5X8=" crossorigin=anonymous media=screen><link rel=icon type=image/svg+xml href=/images/my-favicon.svg sizes=any><link rel=icon type=image/png href=/images/my-favicon-32.ico sizes=32x32><link rel=icon type=image/png href=/images/my-favicon-16.ico sizes=16x16><link rel=apple-touch-icon href=/images/apple-touch-icon.png><link rel=apple-touch-icon sizes=180x180 href=/images/apple-touch-icon.png><link rel=manifest href=/site.webmanifest><link rel=mask-icon href=/images/safari-pinned-tab.svg color=#5bbad5></head><body class="preload-transitions colorscheme-auto"><div class=float-container><a id=dark-mode-toggle class=colorscheme-toggle><i class="fa fa-adjust fa-fw" aria-hidden=true></i></a></div><main class=wrapper><nav class=navigation><section class=container><a class=navigation-title href=/ja>金沢 直晃</a>
<input type=checkbox id=menu-toggle>
<label class="menu-button float-right" for=menu-toggle><i class="fa fa-bars fa-fw" aria-hidden=true></i></label><ul class=navigation-list><li class=navigation-item><a class=navigation-link href=/ja/about/>自己紹介</a></li><li class=navigation-item><a class=navigation-link href=/ja/publications/>成果</a></li><li class=navigation-item><a class=navigation-link href=/ja/posts/>投稿</a></li><li class=navigation-item><a class=navigation-link href=/ja/contact/>連絡先</a></li><li class="navigation-item menu-separator"><span>|</span></li><li class=navigation-item><a href=/publications/>English</a></li></ul></section></nav><div class=content><section class="container page"><article><header><h1 class=title><a class=title-link href=https://kanazawanaoaki.github.io/ja/publications/>Publications</a></h1></header><h2 id=international-journal-papers>International Journal Papers
<a class=heading-link href=#international-journal-papers><i class="fa fa-link" aria-hidden=true title=見出しへのリンク></i>
<span class=sr-only>見出しへのリンク</span></a></h2><ol><li><p>Y. Obinata, K. Kawaharazuka, <strong>N. Kanazawa</strong>, N. Yamaguchi, N. Tsukamoto, I. Yanokura, S. Kitagawa, K. Okada, M. Inaba.
&ldquo;Situation classification of living environment by daily life support robot using pre-trained large-scale vision-language model&rdquo;, Advanced Robotics (AR), vol.39, no.7, pp.323-337, 2025.
<a href=https://doi.org/10.1080/01691864.2025.2487608 class=external-link target=_blank rel=noopener>Paper Link</a></p></li><li><p><strong>N. Kanazawa</strong>, K. Kawaharazuka, Y. Obinata, K. Okada, M. Inaba.
&ldquo;Real-world cooking robot system from recipes based on food state recognition using foundation models and PDDL&rdquo;, Advanced Robotics (AR), vol.38, no.18, pp.1318-1334, 2024.
<a href=https://doi.org/10.1080/01691864.2024.2407136 class=external-link target=_blank rel=noopener>Paper Link</a> <a href=https://arxiv.org/abs/2410.02874 class=external-link target=_blank rel=noopener>Arxiv Link</a> <a href=https://kanazawanaoaki.github.io/cook-from-recipe-pddl/ class=external-link target=_blank rel=noopener>Project Page</a> <a href="https://youtu.be/3bQRTAKV0wM?si=jCCfpHHxBXbrA2-_" class=external-link target=_blank rel=noopener>Video</a></p></li><li><p>K. Kawaharazuka, Y. Obinata, <strong>N. Kanazawa</strong>, N. Tsukamoto, K. Okada, M. Inaba.
&ldquo;Reflex-Based Open-Vocabulary Navigation without Prior Knowledge Using Omnidirectional Camera and Multiple Vision-Language Models&rdquo;, Advanced Robotics (AR), vol.38, no.18, pp.1307-1317, 2024.
<a href=https://doi.org/10.1080/01691864.2024.2393409 class=external-link target=_blank rel=noopener>Paper Link</a> <a href=https://arxiv.org/abs/2408.11380 class=external-link target=_blank rel=noopener>Arxiv Link</a> <a href=https://haraduka.github.io/omnidirectional-vlm/ class=external-link target=_blank rel=noopener>Project Page</a> <a href="https://youtu.be/T2Uezkpu5u4?si=Jf6yMUZMsdIFU0E8" class=external-link target=_blank rel=noopener>Video</a></p></li><li><p>K. Kawaharazuka, Y. Obinata, <strong>N. Kanazawa</strong>, K. Okada, M. Inaba.
&ldquo;Robotic Environmental State Recognition with Pre-Trained Vision-Language Models and Black-Box Optimization&rdquo;, Advanced Robotics (AR), vol.38, no.18, pp.1255-1264, 2024.
<a href=https://doi.org/10.1080/01691864.2024.2366995 class=external-link target=_blank rel=noopener>Paper Link</a> <a href=https://arxiv.org/abs/2409.17519 class=external-link target=_blank rel=noopener>Arxiv Link</a> <a href=https://haraduka.github.io/vlm-bbo/ class=external-link target=_blank rel=noopener>Project Page</a> <a href="https://youtu.be/aOoQcEdVb6M?si=sMOoERErn4VUEE4l" class=external-link target=_blank rel=noopener>Video</a></p></li><li><p>K. Kawaharazuka, <strong>N. Kanazawa</strong>, Y. Obinata, K. Okada, M. Inaba.
&ldquo;Continuous Object State Recognition for Cooking Robots Using Pre-Trained Vision-Language Models and Black-box Optimization&rdquo;, IEEE Robotics and Automation Letters (RAL), vol.9, no.5, pp.4059-4066, 2024. (presented at Humanoids2024),
<a href=https://doi.org/10.1109/LRA.2024.3375257 class=external-link target=_blank rel=noopener>Paper Link</a> <a href=https://arxiv.org/abs/2403.08239 class=external-link target=_blank rel=noopener>Arxiv Link</a> <a href=https://haraduka.github.io/continuous-state-recognition/ class=external-link target=_blank rel=noopener>Project Page</a> <a href="https://www.youtube.com/watch?v=480caUHXrE0" class=external-link target=_blank rel=noopener>Video</a></p></li><li><p>K. Kawaharazuka, <strong>N. Kanazawa</strong>, K. Okada, M. Inaba. &ldquo;Self-Supervised Learning of Visual Servoing for Low-Rigidity Robots Considering Temporal Body Changes&rdquo;, IEEE Robotics and Automation Letters (RAL), vol.7, no.3, pp.7881-7887, 2022. <strong>SICE International Young Authors Award (SIYA-IROS2022)</strong>, (presented at IROS2022),
<a href=https://ieeexplore.ieee.org/document/9806167/ class=external-link target=_blank rel=noopener>Paper Link</a> <a href=https://arxiv.org/abs/2405.11798 class=external-link target=_blank rel=noopener>Arxiv Link</a> <a href="https://www.youtube.com/watch?v=ulWgQTVDGQA" class=external-link target=_blank rel=noopener>Video</a></p></li></ol><h2 id=international-conference-proceedings-peer-reviewed>International Conference Proceedings (Peer Reviewed)
<a class=heading-link href=#international-conference-proceedings-peer-reviewed><i class="fa fa-link" aria-hidden=true title=見出しへのリンク></i>
<span class=sr-only>見出しへのリンク</span></a></h2><ol><li><p>S. Kim, <strong>N. Kanazawa</strong>, S. Hasegawa, K. Kawaharazuka, K. Okada.
&ldquo;Front Hair Styling Robot System Using Path Planning for Root-Centric Strand Adjustment&rdquo;, in Proceedings of the 2025 IEEE/SICE International Symposium on System Integration (SII2025), 2025. <strong>Best Student Paper Finalist</strong>
<a href=https://ieeexplore.ieee.org/document/10871088/ class=external-link target=_blank rel=noopener>Paper Link</a> <a href=https://arxiv.org/abs/2501.10991 class=external-link target=_blank rel=noopener>Arxive Link</a> <a href="https://www.youtube.com/watch?v=AUBmOXsnqbg" class=external-link target=_blank rel=noopener>Video</a></p></li><li><p>K. Kawaharazuka, Y. Obinata, <strong>N. Kanazawa</strong>, K. Okada, M. Inaba.
&ldquo;Robotic State Recognition with Image-to-Text Retrieval Task of Pre-Trained Vision-Language Model and Black-Box Optimization&rdquo;, in Proceedings of the 2024 IEEE-RAS International Conference on Humanoid Robots (Humanoids2024), 2024.
<a href=https://ieeexplore.ieee.org/document/10769848 class=external-link target=_blank rel=noopener>Paper Link</a> <a href=https://arxiv.org/abs/2410.22707 class=external-link target=_blank rel=noopener>Arxive Link</a></p></li><li><p>Y. Obinata, H. Jia, K. Kawaharazuka, <strong>N. Kanazawa</strong>, K. Okada
&ldquo;Remote Life Support Robot Interface System for Global Task Planning and Local Action Expansion Using Foundation Models&rdquo;, in Proceedings of the 2024 IEEE-RAS International Conference on Humanoid Robots (Humanoids2024), 2024.
<a href=https://ieeexplore.ieee.org/document/10769852 class=external-link target=_blank rel=noopener>Paper Link</a> <a href=https://arxiv.org/abs/2411.10038 class=external-link target=_blank rel=noopener>Arxive Link</a> <a href="https://youtu.be/bM0w69k0LM8?si=YwbjSqW216hyQoUK" class=external-link target=_blank rel=noopener>Video</a></p></li><li><p>Open X-Embodiment Collaboration.
&ldquo;Open X-Embodiment: Robotic Learning Datasets and RT-X Models&rdquo;, in Proceedings of the 2024 IEEE International Conference on Robotics and Automation (ICRA2024), 2024. <strong>Best Conference Paper Award</strong>, <strong>Finalists of Best Paper Award in Robot Manipulation</strong>
<a href=https://doi.org/10.1109/ICRA57147.2024.10611477 class=external-link target=_blank rel=noopener>Paper Link</a> <a href=https://robotics-transformer-x.github.io/paper.pdf class=external-link target=_blank rel=noopener>Arxiv Link</a> <a href=https://robotics-transformer-x.github.io/ class=external-link target=_blank rel=noopener>Project Page</a></p></li><li><p>K. Kawaharazuka, Y. Obinata, <strong>N. Kanazawa</strong>, K. Okada, M. Inaba.
&ldquo;Robotic Applications of Pre-Trained Vision-Language Models to Various Recognition Behaviors&rdquo;, in Proceedings of the 2023 IEEE-RAS International Conference on Humanoid Robots (Humanoids2023), pp.458-465, 2023.
<a href=https://doi.org/10.1109/Humanoids57100.2023.10375211 class=external-link target=_blank rel=noopener>Paper Link</a> <a href=https://arxiv.org/abs/2303.05674 class=external-link target=_blank rel=noopener>Arxiv Link</a></p></li><li><p>K. Kawaharazuka, <strong>N. Kanazawa</strong>, Y. Obinata, K. Okada, M. Inaba.
&ldquo;Daily Assistive View Control Learning of Low-Cost Low-Rigidity Robot via Large-Scale Vision-Language Model&rdquo;, in Proceedings of the 2023 IEEE-RAS International Conference on Humanoid Robots (Humanoids2023), pp.452-457, 2023.
<a href=https://doi.org/10.1109/Humanoids57100.2023.10375239 class=external-link target=_blank rel=noopener>Paper Link</a> <a href=https://arxiv.org/abs/2312.07451 class=external-link target=_blank rel=noopener>Arxiv Link</a></p></li><li><p>Y. Obinata, K. Kawaharazuka, <strong>N. Kanazawa</strong>, N. Yamaguchi, N. Tsukamoto, I. Yanokura, S. Kitagawa, K. Shinjo, K. Okada, M. Inaba.
&ldquo;Semantic Scene Difference Detection in Daily Life Patroling by Mobile Robots Using Pre-Trained Large-Scale Vision-Language Model&rdquo;, in Proceedings of the 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS2023), pp.3228-3233, 2023.
<strong>IEEE RAS Japan Joint Chapter Young Award (2023)</strong>, <strong>SICE International Young Authors Award (SIYA-IROS2023)</strong>
<a href=https://doi.org/10.1109/IROS55552.2023.10342467 class=external-link target=_blank rel=noopener>Paper Link</a> <a href=https://arxiv.org/abs/2309.16552 class=external-link target=_blank rel=noopener>Arxiv Link</a></p></li><li><p><strong>N. Kanazawa</strong>, K. Kawaharazuka, Y. Obinata, K. Okada, M. Inaba.
&ldquo;Recognition of Heat-Induced Food State Changes by Time-Series Use of Vision-Language Model for Cooking Robot&rdquo;, in Proceedings of the 18th International Conference on Intellignet Autonomous Systems (IAS2023), pp.547-560, 2023.
<a href=https://doi.org/10.1007/978-3-031-44851-5_42 class=external-link target=_blank rel=noopener>Paper Link</a> <a href=https://arxiv.org/abs/2309.01528 class=external-link target=_blank rel=noopener>Arxiv Link</a></p></li><li><p>K. Kawaharazuka, Y. Obinata, <strong>N. Kanazawa</strong>, K. Okada, M. Inaba.
&ldquo;VQA-based Robotic State Recognition Optimized with Genetic Algorithm&rdquo;, in Proceedings of the 2023 IEEE International Conference on Robotics and Automation (ICRA2023), pp.8306-8311, 2023.
<a href=https://doi.org/10.1109/ICRA48891.2023.10160390 class=external-link target=_blank rel=noopener>Paper Link</a> <a href=https://arxiv.org/abs/2303.05052 class=external-link target=_blank rel=noopener>Arxiv Link</a></p></li><li><p>K. Kawaharazuka, <strong>N. Kanazawa</strong>, K. Okada, M. Inaba.
&ldquo;Learning-Based Wiping Behavior of Low-Rigidity Robots Considering Various Surface Materials and Task Definitions&rdquo;, in Proceedings of the 2022 IEEE-RAS International Conference on Humanoid Robots (Humanoids2022), pp.919-924, 2022.
<a href=https://doi.org/10.1109/Humanoids53995.2022.10000172 class=external-link target=_blank rel=noopener>Paper Link</a> <a href=https://arxiv.org/abs/2403.11198 class=external-link target=_blank rel=noopener>Arxiv Link</a> <a href="https://www.youtube.com/watch?v=N47IXZ6Q0io" class=external-link target=_blank rel=noopener>Video</a></p></li></ol><h2 id=international-workshop>International Workshop
<a class=heading-link href=#international-workshop><i class="fa fa-link" aria-hidden=true title=見出しへのリンク></i>
<span class=sr-only>見出しへのリンク</span></a></h2><ol><li><p>K. Kawaharazuka, Y. Obinata, <strong>N. Kanazawa</strong>, N. Tsukamoto, K. Okada.
&ldquo;Reflexive Open-Vocabulary Navigation without Prior Knowledge Using Omnidirectional Camera and Multiple Vision-Language Models&rdquo;, 2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS2024), 2024, (Workshop on Environment Dynamics Matters: Embodied Navigation to Movable Objects) <strong>Excellent Practice Award</strong></p></li><li><p>Open X-Embodiment Collaboration.
&ldquo;Open X-Embodiment: Robotic Learning Datasets and RT-X Models&rdquo;, Proceedings of the 2023 Neural Information Processing Systems (NeurIPS2023), 2023, (6th Robot Learning Workshop: Pretraining, Fine-Tuning, and Generalization with Large Scale Models)</p></li><li><p>Open X-Embodiment Collaboration.
&ldquo;Open X-Embodiment: Robotic Learning Datasets and RT-X Models&rdquo;, Proceedings of the 2023 Conference on Robot Learning (CoRL2023), 2023, (2nd Workshop on Language and Robot Learning: Language as Grounding)</p></li><li><p>Open X-Embodiment Collaboration.
&ldquo;Open X-Embodiment: Robotic Learning Datasets and RT-X Models&rdquo;, Proceedings of the 2023 Conference on Robot Learning (CoRL2023), 2023, (Towards Generalist Robots: Learning Paradigms for Scalable Skill Acquisition)</p></li></ol><h2 id=arxiv>arXiv
<a class=heading-link href=#arxiv><i class="fa fa-link" aria-hidden=true title=見出しへのリンク></i>
<span class=sr-only>見出しへのリンク</span></a></h2><ol><li><p>K. Kawaharazuka, Y. Obinata, <strong>N. Kanazawa</strong>, K. Okada, M. Inaba.
&ldquo;Binary State Recognition by Robots using Visual Question Answering of Pre-Trained Vision-Language Model&rdquo;, arXiv preprint arXiv:2310.16405, 2023.
<a href=https://arxiv.org/abs/2310.16405 class=external-link target=_blank rel=noopener>Arxiv Link</a></p></li><li><p>Y. Obinata, <strong>N. Kanazawa</strong>, K. Kawaharazuka, I. Yanokura, S. Kim, K. Okada, M. Inaba.
&ldquo;Foundation Model based Open Vocabulary Task Planning and Executive System for General Purpose Service Robots&rdquo;, arXiv preprint arXiv:2308.03357, 2023.
<a href=https://arxiv.org/abs/2308.03357 class=external-link target=_blank rel=noopener>Arxiv Link</a> <a href="https://www.youtube.com/watch?app=desktop&v=fiN4Zibk6Sg" class=external-link target=_blank rel=noopener>Video</a></p></li></ol><h2 id=domestic-journal-papers>Domestic Journal Papers
<a class=heading-link href=#domestic-journal-papers><i class="fa fa-link" aria-hidden=true title=見出しへのリンク></i>
<span class=sr-only>見出しへのリンク</span></a></h2><ol><li><p>河原塚 健人, 大日方 慶樹, <strong>金沢 直晃</strong>, 岡田 慧, 稲葉 雅幸.
大規模視覚-言語モデルと遺伝的アルゴリズムに基づくロボットのための状態認識, 日本ロボット学会誌 (JRSJ), vol.42, no.3, pp.259-265, 2024.
<a href=https://www.rsj.or.jp/pub/jrsj/advpub/231213-04.html class=external-link target=_blank rel=noopener>Paper Link</a></p></li><li><p><strong>金沢 直晃</strong>, 河原塚 健人, 大日方 慶樹, 岡田 慧, 稲葉 雅幸.
料理レシピ記述解析と視覚 - 言語モデルの時系列利用による食材状態変化認識に基づくロボットの調理作業実行, 日本ロボット学会誌 (JRSJ), vol.42, no.3, pp.266-273, 2024.
<a href=https://www.rsj.or.jp/pub/jrsj/advpub/231213-05.html class=external-link target=_blank rel=noopener>Paper Link</a></p></li></ol><h2 id=domestic-conference-proceedings-peer-reviewed>Domestic Conference Proceedings (Peer Reviewed)
<a class=heading-link href=#domestic-conference-proceedings-peer-reviewed><i class="fa fa-link" aria-hidden=true title=見出しへのリンク></i>
<span class=sr-only>見出しへのリンク</span></a></h2><ol><li><p>河原塚 健人, 大日方 慶樹, <strong>金沢 直晃</strong>, 岡田 慧, 稲葉 雅幸.
視覚-言語モデルと遺伝的アルゴリズムに基づくロボットのための離散・連続状態認識, in 第28回ロボティクスシンポジア予稿集 (ROBOSYM2023), pp. 34-35, 2023.</p></li><li><p>大日方 慶樹, 河原塚 健人, <strong>金沢 直晃</strong>, 山口 直也, 塚本 直人, 矢野倉 伊織, 北川 晋吾, 岡田 慧, 稲葉 雅幸.
事前学習済み視覚-言語モデルを用いた巡回ロボットの長期記憶に基づく日常環境の状況分類, in 第28回ロボティクスシンポジア予稿集 (ROBOSYM2023), pp. 36-37, 2023.</p></li><li><p><strong>金沢 直晃</strong>, 河原塚 健人, 大日方 慶樹, 岡田 慧, 稲葉 雅幸.
調理支援ロボットの視覚-言語モデル時系列利用によるレシピ記述からの食材状態変化認識, in 第28回ロボティクスシンポジア予稿集 (ROBOSYM2023), pp. 66-67, 2023. <strong>第13回ロボティクスシンポジア研究奨励賞</strong>.</p></li></ol><h2 id=domestic-conference-proceedings-no-reviewed>Domestic Conference Proceedings (No Reviewed)
<a class=heading-link href=#domestic-conference-proceedings-no-reviewed><i class="fa fa-link" aria-hidden=true title=見出しへのリンク></i>
<span class=sr-only>見出しへのリンク</span></a></h2><ol><li>河原塚 健人, 大日方 慶樹, <strong>金沢 直晃</strong>, 塚本 直人, 岡田 慧.
全天球カメラと事前学習済み視覚-言語モデルによる事前知識を必要としない反射型Open Vocabulary Navigation, in 第25回SICEシステムインテグレーション部門講演会概要集 (SI2024), 1F6-04, 2024, <strong>優秀講演賞</strong></li><li>河原塚 健人, <strong>金沢 直晃</strong>, 大日方 慶樹, 岡田 慧.
大規模視覚-言語モデルによる調理ロボットの時系列食材状態認識, in 第42回日本ロボット学会学術講演会講演論文集 (RSJ2024), 3D2-03, 2024.</li><li>大日方 慶樹, 賈 浩宇, 河原塚 健人, <strong>金沢 直晃</strong>, 岡田 慧.
あいまいな生活支援ロボット動作記述のVLMとARデバイスを用いた提示と指示による展開, in 第42回日本ロボット学会学術講演会講演論文集 (RSJ2024), 3D3-02, 2024.</li><li>大日方 慶樹, 塚本 直人, 河原塚 健人, <strong>金沢 直晃</strong>, 岡田 慧, 稲葉 雅幸.
生活支援ロボットの現場知識に基づくオンライン動作プログラム展開, in 第38回人工知能学会全国大会講演論文集 (JSAI2024), 4E1-GS-8-04, 2024.</li><li><strong>金沢 直晃</strong>, 河原塚 健人, 大日方 慶樹, 岡田 慧, 稲葉 雅幸.
基盤モデルと古典プランニングを用いたレシピ記述からの実世界調理計画認識実行ロボットシステム, in 言語処理学会第30回年次大会 (NLP2024), E1-3, 2024.</li><li><strong>金沢 直晃</strong>, 河原塚 健人, 大日方 慶樹, 岡田 慧, 稲葉 雅幸.
対象物状態中心の調理行動記述に基づくレシピからの卵料理の実世界調理実行ロボットシステム,
in 第24回SICEシステムインテグレーション部門講演会講演概要集 (SI2023), 3G2-08, 2023. <strong>優秀講演賞</strong></li><li>金 淳暁, <strong>金沢 直晃</strong>, 長谷川 峻, 河原塚 健人, 岡田 慧, 稲葉 雅幸.
生活支援ロボットを用いた視覚と力覚に基づく頭髪ブラッシング動作生成に関する研究,
in 第24回SICEシステムインテグレーション部門講演会講演概要集 (SI2023), 3F1-08, 2023.</li><li><strong>金沢 直晃</strong>, 河原塚 健人, 大日方 慶樹, 岡田 慧, 稲葉 雅幸.
調理ロボットのための基盤モデル利用によるレシピ記述からの卵料理の食材状態変化認識と動作シーケンス生成,
in 第41回日本ロボット学会学術講演会講演論文集 (RSJ2023), 1K3-03, 2023.</li><li>大日方 慶樹, <strong>金沢 直晃</strong>, 河原塚 健人, 矢野倉 伊織, 金 淳暁, 岡田 慧, 稲葉 雅幸.
大規模言語モデルによるタスク実行管理器生成法とRoboCup JapanOpen @Home League GPSRタスクへの応用,
in 第41回日本ロボット学会学術講演会講演論文集 (RSJ2023), 1K4-05, 2023. <strong>日本ロボット学会第5回優秀講演賞</strong></li><li><strong>金沢 直晃</strong>, 河原塚 健人, 石田 寛和, 岡田 慧, 稲葉 雅幸.
反復自動データ収集を用いた模倣学習による環境設備操作タスクの実現,
in 日本機械学会ロボティクス・メカトロニクス講演会'23 講演論文集 (ROBOMECH2023), 1P1-D05, 2023.</li><li>大日方 慶樹, 河原塚 健人, <strong>金沢 直晃</strong>, 山口 直也, 塚本 直人, 矢野倉 伊織, 北川 晋吾, 岡田 慧, 稲葉 雅幸.
大規模視覚-言語モデルとチャットインタフェースを用いた生活環境の分類とロボットタスクマッピングシステム,
in 日本機械学会ロボティクス・メカトロニクス講演会'23 講演論文集 (ROBOMECH2023), 1P1-D06, 2023.</li><li>河原塚 健人, 大日方 慶樹, <strong>金沢 直晃</strong>, 岡田 慧, 稲葉 雅幸.
日常生活支援ロボットに向けた大規模視覚-言語モデルと進化的計算に基づく状態認識,
in 第37回人工知能学会全国大会講演論文集 (JSAI2023), 3G1-OS-24a-04, 2023.</li><li><strong>金沢 直晃</strong>, 河原塚 健人, 大日方 慶樹, 岡田 慧, 稲葉 雅幸.
大規模基盤モデル利用による料理レシピ記述からの食材状態変化を考慮した調理認識計画行動ロボットシステム,
in 第37回人工知能学会全国大会講演論文集 (JSAI2023), 3G1-OS-24a-02, 2023.</li><li>河原塚 健人, <strong>金沢 直晃</strong>, 岡田 慧, 稲葉 雅幸.
低剛性ロボットの身体変化を考慮した自律的視覚サーボ学習,
in 第23回SICEシステムインテグレーション部門講演会講演概要集 (SI2022), 3P2-H07, 2022.</li><li><strong>金沢 直晃</strong>, 山口 直也, 北川 晋吾, 岡田 慧, 稲葉 雅幸.
小型センシングモジュールによる知能化家電とロボットが連携して食関連生活支援を行うマルチエージェント型ハウスキーピングシステム,
in 第23回SICEシステムインテグレーション部門講演会講演概要集 (SI2022), 3P2-B18, 2022.</li><li><strong>金沢 直晃</strong>, 河原塚 健人, 石田 寛和, 岡田 慧, 稲葉 雅幸.
ロボットの反復 pick-and-place 自動データ収集によるOne-Shot 教示把持動作スキル学習システム,
in 第40回日本ロボット学会学術講演会講演論文集 (RSJ2022), 3F1-03, 2022.</li><li><strong>金沢 直晃</strong>, 河原塚 健人, 岡田 慧, 稲葉 雅幸.
Parametric Bias を用いた食材特徴を考慮可能な調理ロボットの包丁切断操作学習,
in 第40回日本ロボット学会学術講演会講演論文集 (RSJ2022), 4I1-06, 2022.</li><li><strong>金沢 直晃</strong>, 河原塚 健人, 岡田 慧, 稲葉 雅幸.
Parametric Biasを用いた調理ロボットの包丁切断操作における食材特徴学習,
in 日本機械学会ロボティクス・メカトロニクス講演会'22 講演論文集 (ROBOMECH2022), 1A1-T11, 2022.</li><li><strong>金沢 直晃</strong>, 北川 晋吾, 室岡 貴之, 岡田 慧, 稲葉 雅幸.
テンプレートマッチング認識とプリミティブ動作により状態を考慮した家電操作を行うロボットシステム,
in 第22回SICEシステムインテグレーション部門講演会講演概要集 (SI2021), 1G2-02, 2021. <strong>優秀講演賞</strong></li><li><strong>金沢 直晃</strong>, 岡田 慧, 稲葉 雅幸.
台車移動型双腕ロボットによるカレー調理行動実行システム,
in 第39回日本ロボット学会学術講演会講演論文集 (RSJ2021), 2H3-05, 2021.</li><li><strong>金沢 直晃</strong>, 北川 晋吾, 室岡 貴之, 岡田 慧, 稲葉 雅幸.
調理道具を扱う双腕ロボットによる野菜皮剥き切断操作の認識行動実現システム,
in 第21回SICEシステムインテグレーション部門講演会講演概要集 (SI2020), 3E2-08, 2020.</li></ol><h2 id=awards-publication>Awards (Publication)
<a class=heading-link href=#awards-publication><i class="fa fa-link" aria-hidden=true title=見出しへのリンク></i>
<span class=sr-only>見出しへのリンク</span></a></h2><ol><li><p>S. Kim, <strong>N. Kanazawa</strong>, S. Hasegawa, K. Kawaharazuka, K. Okada.
<strong>Best Student Paper Finalist</strong>, 2025 IEEE/SICE International Symposium on System Integration (SII2025), 2025.1.24.</p></li><li><p>河原塚 健人, 大日方 慶樹, <strong>金沢 直晃</strong>, 塚本 直人, 岡田 慧.
<strong>優秀講演賞</strong>, 第25回SICEシステムインテグレーション部門講演会 (SI2024), 2024.12.20.</p></li><li><p>K. Kawaharazuka, Y. Obinata, <strong>N. Kanazawa</strong>, N. Tsukamoto, K. Okada.
<strong>Excellent Practice Award</strong>, 2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS2024), (Workshop on Environment Dynamics Matters: Embodied Navigation to Movable Objects), 2024.10.14.</p></li><li><p>Open X-Embodiment Collaboration
<strong>Finalists of Best Paper Award in Robot Manipulation</strong>, Proceedings of the 2024 IEEE International Conference on Robotics and Automation (ICRA2024), 2024.5.16.</p></li><li><p>Open X-Embodiment Collaboration
<strong>Best Conference Paper Award</strong>, Proceedings of the 2024 IEEE International Conference on Robotics and Automation (ICRA2024), 2024.5.16.</p></li><li><p><strong>金沢 直晃</strong>, 河原塚 健人, 大日方 慶樹, 岡田 慧, 稲葉 雅幸.
<strong>優秀講演賞</strong>, 第24回SICEシステムインテグレーション部門講演会講演概要集 (SI2023), 2023.12.16.</p></li><li><p><strong>金沢 直晃</strong>.
<strong>第13回ロボティクスシンポジア研究奨励賞</strong>, 第28回ロボティクスシンポジア (ROBOSYM2023), 2023.9.13.</p></li><li><p><strong>金沢 直晃</strong>, 北川 晋吾, 室岡 貴之, 岡田 慧, 稲葉 雅幸.
<strong>優秀講演賞</strong>, 第22回 計測自動制御学会 システムインテグレーション部門講演会 (SI2021) 2021.12.24.</p></li></ol><h2 id=awards-and-experiences-others>Awards and Experiences (Others)
<a class=heading-link href=#awards-and-experiences-others><i class="fa fa-link" aria-hidden=true title=見出しへのリンク></i>
<span class=sr-only>見出しへのリンク</span></a></h2><ol><li>Team JSK: Y. Obinata, <strong>N. Kanazawa</strong>, S. Kim, K. Kawaharazuka, I. Yanokura, S. Kitagawa. <strong>First Place (GPSR task in DSPL)</strong>, RoboCup@Home JapanOpen2022, 2023.03.06-09.</li><li>Team 4: 稲富翔伍, <strong>金沢直晃</strong>, 牧原昂志, 籾山陽紀, 渡部泰樹(五十音順). &ldquo;柔軟物体操作のための世界モデルの構築と実環境への適用可能性の検証&rdquo;, <strong>最優秀賞</strong>, 東京大学 集中講義 世界モデルと知能 最終課題発表会 口頭発表, 2022.05.25. <a href=https://world-model.t.u-tokyo.ac.jp/ class=external-link target=_blank rel=noopener>Link</a></li><li>SFP2019黒板消しロボットチーム, <strong>チームラボ賞</strong>, GUGEN2019.12.08. <a href=https://gugen.jp/result/2019.html class=external-link target=_blank rel=noopener>Link</a>, <a href=https://gugen.jp/subscriptions/work/871 class=external-link target=_blank rel=noopener>Product Link</a></li><li><strong>twilio 賞</strong>, TRUNK HACKATHON in OSAKA 2019. 2019.03.16-17.</li></ol></article></section></div><footer class=footer><section class=container>©
2023 -
2025
金沢 直晃
·
Powered by <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a> & <a href=https://github.com/luizdepra/hugo-coder/ target=_blank rel=noopener>Coder</a>.</section></footer></main><script src=/js/coder.min.6ae284be93d2d19dad1f02b0039508d9aab3180a12a06dcc71b0b0ef7825a317.js integrity="sha256-auKEvpPS0Z2tHwKwA5UI2aqzGAoSoG3McbCw73gloxc="></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-P3RNNXY2FH"></script>
<script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-P3RNNXY2FH")</script></body></html>